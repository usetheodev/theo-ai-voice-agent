"""
Pipeline de Conversa√ß√£o: STT ‚Üí LLM ‚Üí TTS

Arquitetura refatorada com suporte a:
- Providers ass√≠ncronos com lifecycle (connect/disconnect)
- Warmup para eliminar cold-start
- Streaming para menor lat√™ncia
- M√©tricas e health checks

Modos de opera√ß√£o:
- Batch: Processa todo o √°udio de uma vez
- Stream: Processa e retorna √°udio incrementalmente
"""

import asyncio
import logging
import queue
import threading
import time
from typing import Optional, Tuple, Generator, AsyncGenerator

from config import AUDIO_CONFIG, AGENT_MESSAGES, PIPELINE_CONFIG
from providers.stt import create_stt_provider_sync, STTProvider
from providers.llm import create_llm_provider, LLMProvider
from providers.tts import create_tts_provider_sync, TTSProvider
from metrics import (
    track_component_latency,
    track_pipeline_latency,
    track_pipeline_error,
    PIPELINE_LATENCY,
)

logger = logging.getLogger("ai-agent.pipeline")


class ConversationPipeline:
    """
    Pipeline de conversa√ß√£o: STT ‚Üí LLM ‚Üí TTS

    Suporta modo batch e streaming para menor lat√™ncia.
    Os providers s√£o inicializados de forma s√≠ncrona para compatibilidade,
    mas os m√©todos de transcri√ß√£o e s√≠ntese s√£o executados em threads separadas.
    """

    def __init__(self, auto_init: bool = True):
        """
        Inicializa pipeline.

        Args:
            auto_init: Se True, inicializa providers automaticamente.
                      Se False, chame init_providers() ou init_providers_async() manualmente.
        """
        self.stt: Optional[STTProvider] = None
        self.llm: Optional[LLMProvider] = None
        self.tts: Optional[TTSProvider] = None
        self._loop: Optional[asyncio.AbstractEventLoop] = None

        if auto_init:
            self._init_providers_sync()

    def _init_providers_sync(self):
        """Inicializa provedores de forma s√≠ncrona (compatibilidade).

        NOTA: N√£o tenta conectar/warmup aqui se estiver dentro de um event loop.
        Use init_providers_async() para inicializa√ß√£o completa em contexto async.
        """
        try:
            self.stt = create_stt_provider_sync()
            # Conecta de forma s√≠ncrona apenas se N√ÉO estiver em um event loop
            if hasattr(self.stt, 'connect'):
                try:
                    # Verifica se j√° existe um event loop rodando
                    loop = asyncio.get_running_loop()
                    # Se chegou aqui, tem loop rodando - n√£o podemos usar asyncio.run()
                    # O connect ser√° feito depois via init_providers_async()
                    logger.info("‚úÖ STT criado (connect pendente - use init_providers_async)")
                except RuntimeError:
                    # N√£o h√° loop rodando - podemos usar asyncio.run()
                    asyncio.run(self.stt.connect())
                    asyncio.run(self.stt.warmup())
                    logger.info("‚úÖ STT inicializado e conectado")
        except Exception as e:
            logger.warning(f"STT n√£o dispon√≠vel: {e}")

        try:
            self.llm = create_llm_provider()
            logger.info("‚úÖ LLM inicializado")
        except Exception as e:
            logger.warning(f"LLM n√£o dispon√≠vel: {e}")

        try:
            self.tts = create_tts_provider_sync()
            if hasattr(self.tts, 'connect'):
                try:
                    loop = asyncio.get_running_loop()
                    # Loop rodando - connect ser√° feito depois
                    logger.info("‚úÖ TTS criado (connect pendente - use init_providers_async)")
                except RuntimeError:
                    # N√£o h√° loop - podemos conectar agora
                    asyncio.run(self.tts.connect())
                    asyncio.run(self.tts.warmup())
                    logger.info("‚úÖ TTS inicializado e conectado")
        except Exception as e:
            logger.warning(f"TTS n√£o dispon√≠vel: {e}")

    async def init_providers_async(self):
        """Inicializa provedores de forma ass√≠ncrona (recomendado)."""
        from providers.stt import create_stt_provider
        from providers.tts import create_tts_provider

        try:
            self.stt = await create_stt_provider()
            logger.info("‚úÖ STT inicializado (async)")
        except Exception as e:
            logger.warning(f"STT n√£o dispon√≠vel: {e}")

        try:
            self.llm = create_llm_provider()
            logger.info("‚úÖ LLM inicializado")
        except Exception as e:
            logger.warning(f"LLM n√£o dispon√≠vel: {e}")

        try:
            self.tts = await create_tts_provider()
            logger.info("‚úÖ TTS inicializado (async)")
        except Exception as e:
            logger.warning(f"TTS n√£o dispon√≠vel: {e}")

    async def disconnect(self):
        """Desconecta todos os providers."""
        if self.stt and hasattr(self.stt, 'disconnect'):
            await self.stt.disconnect()
        if self.tts and hasattr(self.tts, 'disconnect'):
            await self.tts.disconnect()
        logger.info("üîå Pipeline desconectado")

    # ==================== Health Check ====================

    async def health_check(self) -> dict:
        """Verifica sa√∫de de todos os providers."""
        health = {"status": "healthy", "providers": {}}

        if self.stt and hasattr(self.stt, 'health_check'):
            result = await self.stt.health_check()
            health["providers"]["stt"] = {
                "status": result.status.value,
                "message": result.message,
                "latency_ms": result.latency_ms,
            }
            if result.status.value != "healthy":
                health["status"] = "degraded"

        if self.tts and hasattr(self.tts, 'health_check'):
            result = await self.tts.health_check()
            health["providers"]["tts"] = {
                "status": result.status.value,
                "message": result.message,
                "latency_ms": result.latency_ms,
            }
            if result.status.value != "healthy":
                health["status"] = "degraded"

        health["providers"]["llm"] = {
            "status": "healthy" if self.llm else "unhealthy",
        }

        return health

    # ==================== M√©tricas ====================

    def get_metrics(self) -> dict:
        """Retorna m√©tricas de todos os providers."""
        metrics = {}

        if self.stt and hasattr(self.stt, 'metrics'):
            m = self.stt.metrics
            metrics["stt"] = {
                "total_requests": m.total_requests,
                "successful_requests": m.successful_requests,
                "failed_requests": m.failed_requests,
                "avg_latency_ms": m.avg_latency_ms,
                "success_rate": m.success_rate,
            }

        if self.tts and hasattr(self.tts, 'metrics'):
            m = self.tts.metrics
            metrics["tts"] = {
                "total_requests": m.total_requests,
                "successful_requests": m.successful_requests,
                "failed_requests": m.failed_requests,
                "avg_latency_ms": m.avg_latency_ms,
                "success_rate": m.success_rate,
            }

        return metrics

    # ==================== Processing (Sync) ====================

    def process(self, audio_data: bytes) -> Tuple[Optional[str], Optional[bytes]]:
        """
        Processa √°udio: STT ‚Üí LLM ‚Üí TTS (modo batch)

        Args:
            audio_data: √Åudio PCM 8kHz mono 16-bit

        Returns:
            Tuple[text_response, audio_response]
        """
        pipeline_start = time.perf_counter()

        # 1. Speech-to-Text
        text = self._transcribe(audio_data)
        if not text:
            logger.debug("STT n√£o detectou fala")
            return None, None

        logger.info(f"üìù Usu√°rio: {text}")

        # 2. LLM
        response = self._generate_response(text)
        logger.info(f"ü§ñ Agente: {response}")

        # 3. Text-to-Speech
        audio_response = self._synthesize(response)

        # Registra lat√™ncia total
        pipeline_elapsed = time.perf_counter() - pipeline_start
        PIPELINE_LATENCY.observe(pipeline_elapsed)
        logger.debug(f"‚è±Ô∏è Pipeline total: {pipeline_elapsed:.2f}s")

        return response, audio_response

    def process_stream(self, audio_data: bytes) -> Generator[Tuple[str, bytes], None, None]:
        """
        Processa √°udio com streaming: STT ‚Üí LLM stream ‚Üí TTS stream

        Yield tuplas (text_chunk, audio_chunk) conforme s√£o gerados.

        Args:
            audio_data: √Åudio PCM 8kHz mono 16-bit

        Yields:
            Tuple[text_chunk, audio_chunk]: Fragmentos de texto e √°udio
        """
        pipeline_start = time.perf_counter()

        # 1. Speech-to-Text (batch)
        text = self._transcribe(audio_data)
        if not text:
            logger.debug("STT n√£o detectou fala")
            return

        logger.info(f"üìù Usu√°rio: {text}")

        # 2+3. LLM ‚Üí TTS streaming (frase por frase)
        if self.llm and self.llm.supports_streaming and self.tts:
            for sentence in self.llm.generate_sentences(text):
                logger.info(f"ü§ñ Agente (frase): {sentence}")

                # Sintetiza a frase
                for audio_chunk in self._synthesize_stream_sync(sentence):
                    yield sentence, audio_chunk
        else:
            # Fallback para modo batch
            response = self._generate_response(text)
            audio_response = self._synthesize(response)
            if audio_response:
                yield response, audio_response

        # Registra lat√™ncia
        pipeline_elapsed = time.perf_counter() - pipeline_start
        PIPELINE_LATENCY.observe(pipeline_elapsed)
        logger.debug(f"‚è±Ô∏è Pipeline stream total: {pipeline_elapsed:.2f}s")

    # ==================== Processing (Async) ====================

    async def process_async(self, audio_data: bytes) -> Tuple[Optional[str], Optional[bytes]]:
        """
        Processa √°udio de forma ass√≠ncrona: STT ‚Üí LLM ‚Üí TTS

        Args:
            audio_data: √Åudio PCM 8kHz mono 16-bit

        Returns:
            Tuple[text_response, audio_response]
        """
        pipeline_start = time.perf_counter()

        # 1. Speech-to-Text (async)
        text = await self._transcribe_async(audio_data)
        if not text:
            logger.debug("STT n√£o detectou fala")
            return None, None

        logger.info(f"üìù Usu√°rio: {text}")

        # 2. LLM (sync in thread)
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, self._generate_response, text)
        logger.info(f"ü§ñ Agente: {response}")

        # 3. Text-to-Speech (async)
        audio_response = await self._synthesize_async(response)

        # Registra lat√™ncia
        pipeline_elapsed = time.perf_counter() - pipeline_start
        PIPELINE_LATENCY.observe(pipeline_elapsed)
        logger.debug(f"‚è±Ô∏è Pipeline async total: {pipeline_elapsed:.2f}s")

        return response, audio_response

    async def process_stream_async(
        self,
        audio_data: bytes
    ) -> AsyncGenerator[Tuple[str, bytes], None]:
        """
        Processa √°udio com streaming REAL ass√≠ncrono usando SentencePipeline.

        O TTS come√ßa a sintetizar assim que o LLM gera a primeira frase.
        Isso reduz significativamente a lat√™ncia percebida.

        Args:
            audio_data: √Åudio PCM 8kHz mono 16-bit

        Yields:
            Tuple[text_chunk, audio_chunk]: Fragmentos de texto e √°udio
        """
        pipeline_start = time.perf_counter()

        # 1. Speech-to-Text (async)
        text = await self._transcribe_async(audio_data)
        if not text:
            logger.debug("STT n√£o detectou fala")
            return

        logger.info(f"üìù Usu√°rio: {text}")

        # 2+3. LLM ‚Üí TTS streaming sentence-level
        if self.llm and self.llm.supports_streaming and self.tts:
            # Usa SentencePipeline para streaming real LLM‚ÜíTTS
            from pipeline.sentence_pipeline import SentencePipeline

            queue_size = PIPELINE_CONFIG.get("sentence_queue_size", 3)
            sentence_pipeline = SentencePipeline(
                llm=self.llm,
                tts=self.tts,
                queue_size=queue_size
            )

            async for sentence, audio_chunk in sentence_pipeline.process_streaming(text):
                yield sentence, audio_chunk

            # Log m√©tricas do pipeline
            metrics = sentence_pipeline.metrics
            logger.info(
                f"üìä SentencePipeline: first_audio={metrics.first_audio_latency_ms:.0f}ms, "
                f"total={metrics.total_latency_ms:.0f}ms"
            )
        else:
            # Fallback para modo batch
            response = await asyncio.get_event_loop().run_in_executor(
                None, self._generate_response, text
            )
            audio_response = await self._synthesize_async(response)
            if audio_response:
                yield response, audio_response

        # Registra lat√™ncia total
        pipeline_elapsed = time.perf_counter() - pipeline_start
        PIPELINE_LATENCY.observe(pipeline_elapsed)
        logger.debug(f"‚è±Ô∏è Pipeline stream async total: {pipeline_elapsed:.2f}s")

    # ==================== Component Methods ====================

    def _transcribe(self, audio_data: bytes) -> Optional[str]:
        """Transcreve √°udio para texto (sync wrapper).

        NOTA: Este m√©todo N√ÉO deve ser chamado de contexto async.
        Use _transcribe_async() em vez disso.
        """
        if not self.stt:
            logger.warning("STT n√£o dispon√≠vel")
            return None

        try:
            with track_component_latency('stt'):
                # Executa m√©todo async de forma s√≠ncrona
                if hasattr(self.stt, 'transcribe'):
                    if asyncio.iscoroutinefunction(self.stt.transcribe):
                        # Verifica se j√° existe loop rodando
                        try:
                            loop = asyncio.get_running_loop()
                            # Loop ativo - n√£o podemos usar asyncio.run()
                            # Cria future e roda no loop existente
                            future = asyncio.run_coroutine_threadsafe(
                                self.stt.transcribe(audio_data), loop
                            )
                            text = future.result(timeout=30)
                        except RuntimeError:
                            # Sem loop - usa asyncio.run()
                            text = asyncio.run(self.stt.transcribe(audio_data))
                    else:
                        text = self.stt.transcribe(audio_data)
                else:
                    return None

            if text and len(text.strip()) >= 2:
                return text.strip()
            return None

        except Exception as e:
            logger.error(f"Erro no STT: {e}")
            track_pipeline_error('stt')
            return None

    async def _transcribe_async(self, audio_data: bytes) -> Optional[str]:
        """Transcreve √°udio para texto (async)."""
        if not self.stt:
            logger.warning("STT n√£o dispon√≠vel")
            return None

        try:
            with track_component_latency('stt'):
                text = await self.stt.transcribe(audio_data)

            if text and len(text.strip()) >= 2:
                return text.strip()
            return None

        except Exception as e:
            logger.error(f"Erro no STT: {e}")
            track_pipeline_error('stt')
            return None

    def _generate_response(self, text: str) -> str:
        """Gera resposta do LLM."""
        if not self.llm:
            return f"Voc√™ disse: {text}"

        try:
            with track_component_latency('llm'):
                return self.llm.generate(text)
        except Exception as e:
            logger.error(f"Erro no LLM: {e}")
            track_pipeline_error('llm')
            return AGENT_MESSAGES["error"]

    def _synthesize(self, text: str) -> Optional[bytes]:
        """Sintetiza texto em √°udio (sync wrapper).

        NOTA: Este m√©todo N√ÉO deve ser chamado de contexto async.
        Use _synthesize_async() em vez disso.
        """
        if not self.tts:
            logger.warning("TTS n√£o dispon√≠vel")
            return None

        try:
            with track_component_latency('tts'):
                if hasattr(self.tts, 'synthesize'):
                    if asyncio.iscoroutinefunction(self.tts.synthesize):
                        # Verifica se j√° existe loop rodando
                        try:
                            loop = asyncio.get_running_loop()
                            # Loop ativo - usa run_coroutine_threadsafe
                            future = asyncio.run_coroutine_threadsafe(
                                self.tts.synthesize(text), loop
                            )
                            return future.result(timeout=60)
                        except RuntimeError:
                            # Sem loop - usa asyncio.run()
                            return asyncio.run(self.tts.synthesize(text))
                    else:
                        return self.tts.synthesize(text)
                return None
        except Exception as e:
            logger.error(f"Erro no TTS: {e}")
            track_pipeline_error('tts')
            return None

    async def _synthesize_async(self, text: str) -> Optional[bytes]:
        """Sintetiza texto em √°udio (async)."""
        if not self.tts:
            logger.warning("TTS n√£o dispon√≠vel")
            return None

        try:
            with track_component_latency('tts'):
                return await self.tts.synthesize(text)
        except Exception as e:
            logger.error(f"Erro no TTS: {e}")
            track_pipeline_error('tts')
            return None

    def _synthesize_stream_sync(self, text: str) -> Generator[bytes, None, None]:
        """Sintetiza texto em √°udio com streaming (sync wrapper).

        NOTA: Este m√©todo √© mantido para compatibilidade, mas prefira
        usar _synthesize_stream_async para streaming real sem bloqueio.
        """
        if not self.tts:
            return

        if hasattr(self.tts, 'synthesize_stream'):
            if asyncio.iscoroutinefunction(self.tts.synthesize_stream):
                # Para async generator em contexto sync, usamos queue
                # para fazer streaming real sem acumular tudo em mem√≥ria
                import queue
                import threading

                chunk_queue: queue.Queue = queue.Queue()

                def _run_async():
                    async def _stream():
                        try:
                            async for chunk in self.tts.synthesize_stream(text):
                                chunk_queue.put(chunk)
                        except Exception as e:
                            logger.error(f"Erro no TTS streaming: {e}")
                        finally:
                            chunk_queue.put(None)  # Sinaliza fim

                    asyncio.run(_stream())

                # Inicia em thread separada
                thread = threading.Thread(target=_run_async, daemon=True)
                thread.start()

                # Yield chunks assim que dispon√≠veis
                while True:
                    chunk = chunk_queue.get()
                    if chunk is None:
                        break
                    yield chunk

                thread.join(timeout=1)
            else:
                # Sync generator - j√° √© streaming real
                for chunk in self.tts.synthesize_stream(text):
                    yield chunk
        else:
            # Fallback para batch
            audio = self._synthesize(text)
            if audio:
                yield audio

    async def _synthesize_stream_async(self, text: str) -> AsyncGenerator[bytes, None]:
        """Sintetiza texto em √°udio com streaming (async)."""
        if not self.tts:
            return

        if hasattr(self.tts, 'synthesize_stream'):
            async for chunk in self.tts.synthesize_stream(text):
                yield chunk
        else:
            # Fallback para batch
            audio = await self._synthesize_async(text)
            if audio:
                yield audio

    # ==================== Greeting / Error ====================

    def generate_greeting(self) -> Tuple[str, Optional[bytes]]:
        """Gera sauda√ß√£o inicial."""
        greeting = AGENT_MESSAGES["greeting"]
        audio = self._synthesize(greeting)
        return greeting, audio

    async def generate_greeting_async(self) -> Tuple[str, Optional[bytes]]:
        """Gera sauda√ß√£o inicial (async)."""
        greeting = AGENT_MESSAGES["greeting"]
        audio = await self._synthesize_async(greeting)
        return greeting, audio

    def generate_greeting_stream(self) -> Generator[Tuple[str, bytes], None, None]:
        """Gera sauda√ß√£o inicial com streaming."""
        greeting = AGENT_MESSAGES["greeting"]

        if self.tts and self.tts.supports_streaming:
            for audio_chunk in self._synthesize_stream_sync(greeting):
                yield greeting, audio_chunk
        else:
            audio = self._synthesize(greeting)
            if audio:
                yield greeting, audio

    async def generate_greeting_stream_async(self) -> AsyncGenerator[Tuple[str, bytes], None]:
        """Gera sauda√ß√£o inicial com streaming (async)."""
        greeting = AGENT_MESSAGES["greeting"]

        if self.tts and self.tts.supports_streaming:
            async for audio_chunk in self._synthesize_stream_async(greeting):
                yield greeting, audio_chunk
        else:
            audio = await self._synthesize_async(greeting)
            if audio:
                yield greeting, audio

    def generate_error_response(self) -> Tuple[str, Optional[bytes]]:
        """Gera resposta de erro."""
        error_msg = AGENT_MESSAGES["error"]
        audio = self._synthesize(error_msg)
        return error_msg, audio

    # ==================== Utils ====================

    def reset(self):
        """Reseta estado da conversa."""
        if self.llm:
            self.llm.reset_conversation()
        logger.info("üîÑ Pipeline resetado")

    @property
    def is_ready(self) -> bool:
        """Verifica se pipeline est√° pronto."""
        return self.stt is not None and self.llm is not None and self.tts is not None

    @property
    def supports_streaming(self) -> bool:
        """Verifica se pipeline suporta streaming completo."""
        llm_stream = self.llm.supports_streaming if self.llm else False
        tts_stream = self.tts.supports_streaming if self.tts else False
        return llm_stream and tts_stream


# ==================== Factory ====================

async def create_pipeline_async() -> ConversationPipeline:
    """
    Factory ass√≠ncrona para criar pipeline com providers inicializados.

    Returns:
        ConversationPipeline com providers conectados e aquecidos.
    """
    pipeline = ConversationPipeline(auto_init=False)
    await pipeline.init_providers_async()
    return pipeline


def create_pipeline() -> ConversationPipeline:
    """
    Factory s√≠ncrona para criar pipeline.

    Returns:
        ConversationPipeline com providers inicializados.
    """
    return ConversationPipeline(auto_init=True)
