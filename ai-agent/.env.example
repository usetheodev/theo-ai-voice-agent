# ==============================================================================
# AI AGENT - CONFIGURAÇÃO DE VARIÁVEIS DE AMBIENTE
# ==============================================================================
#
# Este arquivo documenta TODAS as variáveis de ambiente disponíveis para
# configurar o AI Agent (Conversation Server).
#
# COMO USAR:
# 1. Copie este arquivo para .env
# 2. Ajuste os valores conforme seu ambiente
# 3. Configure as API keys necessárias (ANTHROPIC_API_KEY ou OPENAI_API_KEY)
#
# DICA: Em produção, configure variáveis sensíveis via Docker secrets
# ou variáveis de ambiente do sistema operacional.
#
# ==============================================================================


# ==============================================================================
# WEBSOCKET SERVER
# ==============================================================================
# Configurações do servidor WebSocket que recebe conexões do Media Server.

# Host para bind do servidor WebSocket
# - 0.0.0.0: Aceita conexões de qualquer interface (RECOMENDADO para Docker)
# - 127.0.0.1: Apenas localhost
WS_HOST=0.0.0.0

# Porta do servidor WebSocket
# - Deve ser acessível pelo Media Server
# - Padrão: 8765
WS_PORT=8765

# Número máximo de conexões simultâneas
# - Limita uso de recursos
# - Cada conexão = uma sessão de conversação
# - Recomendado: 10-100 dependendo dos recursos
WS_MAX_CONNECTIONS=100

# Intervalo de ping para manter conexão viva (segundos)
# - Evita timeout por inatividade
# - Recomendado: 20-60 segundos
WS_PING_INTERVAL=30

# Timeout do ping (segundos)
# - Se não receber pong, considera conexão morta
# - Recomendado: 5-15 segundos
WS_PING_TIMEOUT=10

# Timeout para fechar conexão graciosamente (segundos)
# - Tempo máximo para fechamento limpo
# - Recomendado: 3-10 segundos
WS_CLOSE_TIMEOUT=5

# Tamanho máximo de mensagem WebSocket (bytes)
# - Limita tamanho de mensagens recebidas/enviadas
# - 10MB = 10 * 1024 * 1024 = 10485760
# - Suficiente para áudio de até ~10 minutos
WS_MAX_MESSAGE_SIZE=10485760


# ==============================================================================
# LOGGING
# ==============================================================================

# Nível de log da aplicação
# - DEBUG: muito verboso, inclui frames de áudio
# - INFO: operações normais (RECOMENDADO)
# - WARNING: apenas alertas
# - ERROR: apenas erros
# - CRITICAL: apenas erros fatais
LOG_LEVEL=INFO


# ==============================================================================
# MÉTRICAS PROMETHEUS
# ==============================================================================

# Porta do servidor HTTP para métricas
# - Expõe endpoint /metrics para scraping
# - Padrão AI Agent: 9090 (Media Server usa 9091)
METRICS_PORT=9090

# Habilitar servidor de métricas
# - true: inicia servidor HTTP (RECOMENDADO para produção)
# - false: desabilita métricas
METRICS_ENABLED=true


# ==============================================================================
# ÁUDIO - CONFIGURAÇÕES GERAIS
# ==============================================================================
# Parâmetros de áudio para processamento de voz.

# Taxa de amostragem do áudio (Hz)
# - 8000: padrão telefonia PSTN (RECOMENDADO)
# - 16000: wideband
AUDIO_SAMPLE_RATE=8000

# Número de canais (mono=1, stereo=2)
# - Telefonia SIP requer mono
AUDIO_CHANNELS=1

# Largura da amostra em bytes
# - 2: 16-bit PCM (PADRÃO)
AUDIO_SAMPLE_WIDTH=2

# Duração do frame RTP em milissegundos
# - 20ms: padrão VoIP (RECOMENDADO)
AUDIO_FRAME_DURATION_MS=20

# Tamanho máximo do buffer de áudio em segundos
# - Limita consumo de memória
# - 60 segundos = ~960KB por sessão
# - Falas muito longas são truncadas
AUDIO_MAX_BUFFER_SECONDS=60

# Tamanho do chunk de áudio enviado via WebSocket (bytes)
# - 2000 bytes = ~125ms de áudio @ 8kHz
# - Valores menores: menor latência, mais overhead
# - Valores maiores: maior latência, menos pacotes
AUDIO_CHUNK_SIZE_BYTES=2000


# ==============================================================================
# VAD - DETECÇÃO DE ATIVIDADE DE VOZ
# ==============================================================================
# Configurações do Voice Activity Detection.
# CRÍTICO: Determina quando o usuário terminou de falar.

# Threshold de silêncio para fim de fala (milissegundos)
# - Tempo de silêncio para considerar que o usuário parou
# - Pausas naturais: 200-400ms
# - Valores menores (<400ms): mais responsivo, pode cortar frases
# - Valores maiores (>700ms): mais robusto, maior latência
# RECOMENDADO: 500-700ms para português
SILENCE_THRESHOLD_MS=500

# Agressividade do VAD (0-3)
# - 0: muito permissivo, mais falsos positivos
# - 1: permissivo, bom para ambientes silenciosos
# - 2: moderado (RECOMENDADO)
# - 3: agressivo, pode perder fala suave
VAD_AGGRESSIVENESS=2

# Duração mínima de fala para ser válida (milissegundos)
# - Ignora sons muito curtos (ruídos)
# - Palavras curtas ("sim", "não"): 200-300ms
# RECOMENDADO: 200-300ms
VAD_MIN_SPEECH_MS=250

# Threshold de energia RMS para fallback VAD
# - Usado quando webrtcvad não está disponível
# - Depende do nível de ruído ambiente
# - Valores típicos: 300-800
VAD_ENERGY_THRESHOLD=500

# Tamanho do ring buffer do VAD em frames
# - Número de frames para suavização
# - 5 frames @ 20ms = 100ms de janela
VAD_RING_BUFFER_SIZE=5

# Taxa mínima de frames com fala (0.0 - 1.0)
# - 0.4 = 40% dos frames devem ter fala
VAD_SPEECH_RATIO_THRESHOLD=0.4


# ==============================================================================
# ASR/STT - RECONHECIMENTO DE FALA
# ==============================================================================
# Configurações do Speech-to-Text (transcrição de áudio para texto).

# Provider de STT
# - faster-whisper: RECOMENDADO - 4x mais rápido, roda local
# - whisper: Whisper original (mais lento)
# - openai: API OpenAI (requer internet e API key)
ASR_PROVIDER=faster-whisper

# Modelo Whisper
# - tiny: ~1s latência, menor precisão, 39M params (RECOMENDADO para CPU)
# - base: ~2s latência, boa precisão, 74M params
# - small: ~4s latência, melhor precisão, 244M params
# - medium: ~8s latência, alta precisão, 769M params
# - large-v3: ~15s latência, máxima precisão, 1.5B params (requer GPU)
ASR_MODEL=tiny

# Idioma (código ISO-639-1)
# - pt: português
# - en: inglês
# - es: espanhol
# - auto: detectar automaticamente (mais lento)
ASR_LANGUAGE=pt

# Tipo de computação (faster-whisper)
# - int8: quantizado para CPU (RECOMENDADO)
# - float16: para GPU com CUDA
# - float32: máxima precisão
ASR_COMPUTE_TYPE=int8

# Device para processamento
# - cpu: usar CPU (RECOMENDADO para servidores sem GPU)
# - cuda: usar GPU NVIDIA
# - auto: detectar automaticamente
ASR_DEVICE=cpu

# Beam size para transcrição
# - 1: greedy search, mais rápido (RECOMENDADO para latência)
# - 5: beam search, mais preciso, mais lento
ASR_BEAM_SIZE=1

# Habilitar filtro VAD no Whisper
# - false: RECOMENDADO - o media-server já faz VAD
# - true: double-VAD pode causar problemas
ASR_VAD_FILTER=false

# Gerar timestamps de palavras
# - false: RECOMENDADO - não necessário para conversação
# - true: aumenta latência
ASR_WORD_TIMESTAMPS=false

# Número de threads CPU (0 = auto)
# - 0: deixa o sistema decidir
# - 1-N: limita threads
ASR_CPU_THREADS=0

# Número de workers paralelos no modelo
# - 1: RECOMENDADO para baixa latência
# - 2+: pode melhorar throughput com múltiplas requisições
ASR_NUM_WORKERS=1

# Workers no ThreadPoolExecutor
# - Threads para executar transcrições
# - Recomendado: 2
ASR_EXECUTOR_WORKERS=2


# ==============================================================================
# LLM - MODELO DE LINGUAGEM
# ==============================================================================
# Configurações do Large Language Model para gerar respostas.

# Provider de LLM
# - anthropic: Claude (RECOMENDADO para qualidade)
# - openai: GPT
# - local: Docker Model Runner, vLLM, Ollama (RECOMENDADO para latência/custo)
# - mock: respostas fake para testes
LLM_PROVIDER=local
# LLM_PROVIDER=anthropic

# API Key do Anthropic Claude
# - Obrigatório se LLM_PROVIDER=anthropic
# - Obtenha em: https://console.anthropic.com
ANTHROPIC_API_KEY=sk-ant-...

# API Key da OpenAI (alternativo)
# - Obrigatório se LLM_PROVIDER=openai
# - Obtenha em: https://platform.openai.com
# OPENAI_API_KEY=sk-...

# Modelo Anthropic
# - claude-3-haiku-20240307: mais rápido, menor custo (RECOMENDADO)
# - claude-3-sonnet-20240229: equilibrado
# - claude-3-opus-20240229: mais capaz, mais lento
LLM_MODEL=claude-3-haiku-20240307

# Modelo OpenAI (se usar OpenAI)
# - gpt-3.5-turbo: rápido, baixo custo
# - gpt-4: mais capaz, mais lento
OPENAI_LLM_MODEL=gpt-3.5-turbo

# ------------------------------------------------------------------------------
# LLM LOCAL (Docker Model Runner / vLLM / Ollama)
# ------------------------------------------------------------------------------
# Execute modelos de linguagem localmente sem API key, reduzindo latência e custo.
# Requer servidor OpenAI-compatible rodando localmente.
#
# SETUP DOCKER MODEL RUNNER:
#   1. docker desktop enable model-runner --tcp 12434
#   2. docker model pull ai/smollm3
#   3. Configure LLM_PROVIDER=local
#
# SETUP vLLM:
#   1. docker run --gpus all -p 8000:8000 vllm/vllm-openai --model Qwen/Qwen3-0.6B
#   2. Configure LOCAL_LLM_BASE_URL=http://localhost:8000/v1
#
# SETUP OLLAMA:
#   1. ollama serve
#   2. ollama pull llama3
#   3. Configure LOCAL_LLM_BASE_URL=http://localhost:11434/v1

# URL base do servidor local (OpenAI-compatible API)
# - Docker Model Runner: http://localhost:12434/engines/llama.cpp/v1
# - vLLM: http://localhost:8000/v1
# - Ollama: http://localhost:11434/v1
LOCAL_LLM_BASE_URL=http://localhost:12434/engines/llama.cpp/v1

# Modelo local a usar
# Docker Model Runner (do Docker Hub ai/):
#   - ai/functiongemma: 270M params, function-calling, edge (MAIS RAPIDO)
#   - ai/smollm3: 3.1B params, chat eficiente (RECOMENDADO)
#   - ai/phi4: ~3B params, raciocínio compacto
#   - ai/qwen3: 4-72B params, alta qualidade
#   - ai/mistral: modelo eficiente
# vLLM: depende do modelo carregado (ex: Qwen/Qwen3-0.6B)
# Ollama: llama3, mistral, phi3, codellama, etc.
LOCAL_LLM_MODEL=ai/smollm3

# Número máximo de tokens na resposta
# - Limita tamanho da resposta
# - Para telefonia, respostas curtas são melhores
# - Recomendado: 128-256
LLM_MAX_TOKENS=256

# Temperatura (0.0 - 1.0)
# - 0.0: determinístico, respostas consistentes
# - 0.7: criativo mas coerente (RECOMENDADO)
# - 1.0: muito criativo, pode ser inconsistente
LLM_TEMPERATURE=0.7

# Timeout da requisição (segundos)
# - Tempo máximo para aguardar resposta da API
# - Recomendado: 10-20 segundos
LLM_TIMEOUT=15.0

# System prompt customizado (opcional)
# - Define a personalidade e comportamento do agente
# - Use uma linha ou escape newlines com \n
# LLM_SYSTEM_PROMPT=Você é um assistente virtual de atendimento telefônico...


# ==============================================================================
# TTS - SÍNTESE DE VOZ
# ==============================================================================
# Configurações do Text-to-Speech (conversão de texto em áudio).

# Provider de TTS
# - kokoro: neural local, alta qualidade (RECOMENDADO)
# - gtts: Google TTS gratuito, requer internet
# - openai: API OpenAI, alta qualidade, requer API key
# - mock: tom de teste
TTS_PROVIDER=kokoro

# Voz para Kokoro
# Português:
#   - pf_dora: feminino brasileiro (RECOMENDADO)
#   - pm_alex: masculino
#   - pm_santa: masculino
# Inglês Americano:
#   - af_bella, af_nicole, af_sarah, af_sky (feminino)
#   - am_adam, am_michael (masculino)
# Inglês Britânico:
#   - bf_emma, bf_isabella (feminino)
#   - bm_george, bm_lewis (masculino)
TTS_VOICE=pf_dora

# Idioma para gTTS
# - pt: português
# - en: inglês
# - es: espanhol
TTS_LANG=pt

# Sample rate nativo do TTS (Hz)
# - Kokoro gera em 24kHz
# - Será convertido para 8kHz para telefonia
TTS_SAMPLE_RATE=24000

# Sample rate de saída (Hz)
# - 8000 para telefonia padrão
TTS_OUTPUT_SAMPLE_RATE=8000

# Velocidade da fala (0.5 - 2.0)
# - 1.0: velocidade normal (RECOMENDADO)
# - 0.8: mais lento
# - 1.2: mais rápido
TTS_SPEED=1.0

# Workers no ThreadPoolExecutor
# - Threads para síntese de áudio
# - Recomendado: 2
TTS_EXECUTOR_WORKERS=2

# OpenAI TTS (se usar OpenAI)
# OPENAI_TTS_MODEL=tts-1
# OPENAI_TTS_VOICE=alloy


# ==============================================================================
# PIPELINE DE CONVERSAÇÃO
# ==============================================================================
# Configurações do pipeline STT → LLM → TTS.

# Tamanho da fila de frases para streaming
# - Buffer entre LLM e TTS para suavizar streaming
# - Recomendado: 2-5
PIPELINE_SENTENCE_QUEUE_SIZE=3

# Timeout para transcrição STT (segundos)
# - Tempo máximo para aguardar transcrição
# - Recomendado: 20-30 segundos
PIPELINE_STT_TIMEOUT=30.0

# Timeout para síntese TTS (segundos)
# - Tempo máximo para aguardar síntese
# - Recomendado: 30-60 segundos
PIPELINE_TTS_TIMEOUT=60.0

# Timeout para aguardar sentença do LLM (segundos)
# - Tempo máximo para aguardar próxima sentença na fila
# - Recomendado: 20-30 segundos
PIPELINE_SENTENCE_TIMEOUT=30.0


# ==============================================================================
# SESSÕES
# ==============================================================================
# Configurações de gerenciamento de sessões.

# Tempo máximo de inatividade (segundos)
# - Sessões sem atividade são removidas
# - Recomendado: 300-600 segundos (5-10 minutos)
SESSION_MAX_IDLE_SECONDS=300

# Intervalo de limpeza de sessões (segundos)
# - Frequência de verificação de sessões inativas
# - Recomendado: 30-60 segundos
SESSION_CLEANUP_INTERVAL=60


# ==============================================================================
# ESCALAÇÃO AUTOMÁTICA
# ==============================================================================
# Transfere automaticamente para um atendente humano apos N interacoes
# sem resolucao (quando o LLM nao aciona tool calling).

# Numero maximo de interacoes sem resolucao antes de transferir
# - 0 = desabilitado (nunca escala automaticamente)
# - 3 = recomendado para testes
# - 5 = recomendado para producao
MAX_UNRESOLVED_INTERACTIONS=3

# Ramal destino para transferencia automatica
# - Ramal do atendente humano que recebe chamadas escaladas
DEFAULT_TRANSFER_TARGET=1001

# Mensagem falada antes de transferir (enviada ao TTS)
# - O caller ouve essa mensagem antes de ser transferido
DEFAULT_TRANSFER_MESSAGE=Estou te transferindo para outro atendente para que esclareca seus problemas ou duvidas. Aguarde um momento.


# ==============================================================================
# MENSAGENS DO AGENTE
# ==============================================================================
# Mensagens padrão do agente.

# Saudação inicial
# - Primeira mensagem ao atender a chamada
AGENT_GREETING=Olá! Bem-vindo ao atendimento. Como posso ajudá-lo?

# Mensagem de erro
# - Enviada quando ocorre erro no processamento
AGENT_ERROR=Desculpe, tive um problema ao processar sua mensagem.

# Despedida
# - Mensagem de encerramento
AGENT_GOODBYE=Até logo! Tenha um bom dia!
